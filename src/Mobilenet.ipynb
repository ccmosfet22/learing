{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "# from keras.applications import MobileNet\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from keras.optimizers import Adam\n",
    "import os.path\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\YF\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:246: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\YF\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1836: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_model=MobileNetV2(input_shape=(224,224,3),weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "preds=Dense(2,activation='softmax')(x) #final layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=base_model.input,outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x0000015C60A6D688>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x0000015C60A6DEC8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000015C60A6DDC8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000015C60A9ECC8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x0000015C60A9EB08>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x0000015C60A9EEC8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000015C60B2DCC8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x0000015C60B3BD48>\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000015C60BBC208>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000015C60C95E88>\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000015C60CAEAC8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000015C60CE1088>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x0000015C60CE1688>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x0000015C60CF4C88>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x0000015C66E34BC8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000015C66E3B248>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x0000015C66E40E08>\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000015C66E5CE08>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000015C66F8FDC8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000015C66FA3348>\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "# or if we want to set the first 20 layers of the network to be non-trainable\n",
    "for layer in model.layers[:20]:\n",
    "    layer.trainable=False\n",
    "    print(layer)\n",
    "for layer in model.layers[20:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31590 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(\"./training_dataset/\",\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\YF\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3304: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\YF\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "987/987 [==============================] - 356s 361ms/step - loss: 0.0429 - acc: 0.9907\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YF\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:570: RuntimeWarning: Early stopping conditioned on metric `test_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987/987 [==============================] - 306s 311ms/step - loss: 0.0176 - acc: 0.9945\n",
      "Epoch 3/10\n",
      "987/987 [==============================] - 311s 315ms/step - loss: 0.0358 - acc: 0.9923\n",
      "Epoch 4/10\n",
      "987/987 [==============================] - 306s 311ms/step - loss: 0.0174 - acc: 0.9943\n",
      "Epoch 5/10\n",
      "987/987 [==============================] - 313s 317ms/step - loss: 0.0130 - acc: 0.9952\n",
      "Epoch 6/10\n",
      "987/987 [==============================] - 302s 306ms/step - loss: 0.0193 - acc: 0.9941\n",
      "Epoch 7/10\n",
      "987/987 [==============================] - 334s 338ms/step - loss: 0.0130 - acc: 0.9950\n",
      "Epoch 8/10\n",
      "987/987 [==============================] - 315s 319ms/step - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 9/10\n",
      "987/987 [==============================] - 303s 307ms/step - loss: 0.0373 - acc: 0.9923\n",
      "Epoch 10/10\n",
      "987/987 [==============================] - 312s 316ms/step - loss: 0.0219 - acc: 0.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15ddbae5588>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='test_loss', patience=3, verbose=1)\n",
    "# model.fit(x_train,y_train,batch_size=100,epochs=100, validation_data = (x_test, y_test),callbacks = [earlystop])\n",
    "\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Adam optimizer\n",
    "# loss function will be categorical cross entropy\n",
    "# evaluation metric will be accuracy\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=10,\n",
    "                   callbacks = [earlystop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_image(img_path, show=False):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "    img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(img_tensor[0])                           \n",
    "        plt.axis('on')\n",
    "        plt.show()\n",
    "\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAYaklEQVR4nO3de5BU9Z338fd3hoso4IXIMEH24RK1QFEIl00CMV6JwLooKQ1EV3QpxlQpouyawGL2oZ6QKsMT5NF6kigWVnTl5hNFiKJIiKxSPIqgSAS5jAzCMCM3CVeBmenv/tFnZnuYHpjpC6eb83lV/aq7f93n9Pdwaj78zunu8zN3R0SiqyDsAkQkXAoBkYhTCIhEnEJAJOIUAiIRpxAQibishYCZ3Wpmm82s1MwmZet9RCQ9lo3vCZhZIbAFuAUoBz4ERrv7xoy/mYikJVsjgYFAqbtvc/eTwHxgRJbeS0TS0CJL6+0M7Ex4XA78fWMvNrOsfG2xR48e7N27l0OHDmVj9SL5Zp+7X3pqZ7ZGApakr94fupmVmNkaM1vT2EqKi4uJxWJ8+umnAHznO99hxYoVFBYWUlBQgJlRWFiImdXdr21z586ltLSUgwcPcv3117N69Wr69OkDUO91tRL7ampq6j0nco74Immvu2e8Ad8FliY8ngxMPs3rPVkrLi72srIyb9Wqlbdq1coHDx7stV599VW/77773N39qaee8l/84hfeFFdddZVv37697nGHDh28VatWfuLEibq+WCzmx44dS1qTWu60Vq1ahV5DnrU1yf7+snVisAXxE4M3AbuInxj8ibtvaOT1SYsoLi6moqIi6XtUV1dTU1ND69atm1Xb119/TevWrSkoaHwQ5O60bdu23uOvv/66We8j2XfixAnOP/98ampqwi4lX6x19/6ndmblcMDdq4GHgKXAZ8DLjQVAqpYsWcLDDz/c7OUGDx5MeXk5hw4dIhaL1fUfPHiwdlTCoUOHqKio4MiRI1RUVLB79+6M1S3pa9euHe3btweou5U0ZONwIIXDh6TDl+Li4iYN8VPRqVMn37dvX93jli1belVVle/Zs8cBLygo8IqKCj/vvPP80KFDYQ/j1MAvueQSNzPftm2bx2Kxun0XjCTVztySHg5E9huDRUVFDQ4JKisrKSoqAiAWi9GvXz86deoURnlyio4dO7Jx40Z69uzJoEGDKCsro6KiotHDRWmGbP3v3pxGI8mVzZHAqS6//PJ6711QUFD3nEYC4bf33nvPq6qq6vZJmzZtQq8pD9u5OxL46quv2Lt3b0rLbtmyhVgsxp/+9KcGI4PNmzcTi8XYsmVLJsqUFHXr1o0bbrihwT6+4oorQqro3JJXIXD06FFKS0sb9L/55pvMnz+/WevasGED69ato2/fvhw/fpx77rmH3r17c+2113LttdfSu3dv7r77bqqqqrj//vsztQmSgjfffJObb76Zli1b1vVdc801zJkz57Sf8kgTNTZEP5uNRoYvpx4OrFq1yn/wgx80GMpXVFT4qlWrvLy83MvKyvzLL79sdNj/8ccf+6pVq7xDhw5173P06FF///33651sSqTDgXDbpk2bGuyTDz74wFu0aBF6bXnW8v9woH379nTt2pX169fX61+wYAHf+973+MMf/sDrr7/O6tWrG13HjBkzmDx5MocPHwbiHxkWFBTw+OOP131k6O6sWLGCWCzGihUreO+997K3UXJa/fv35/zzz2/Qf/3111NdXR1CReegsEcBzRkJuLu/88473r9/f1+7dm1d38yZMx3we++919944w3/4osvkv6P/u677/rhw4fd3f0nP/mJDx061I8ePVrvNUuWLPE33njDCwoKvKqqyocOHeq33HJL2Ake2fbb3/62bp8lGjFihD4abH5LOhLI1g+Ismrnzp0sX76cDh06sH//fgCuvPJKjhw5woQJE3jssccoKSlpsNzChQvp0aMHbdu2Zc6cOUnXPXv27Lp/nEWLFlFSUsJNN92kL6WE5MEHH6SmpoZvfvObDB8+nPPOOw+A++67j9dff13fFsyEsEcBzRkJ7NmzxydPnuzXXHONr1ixwl988UUvKSnxmTNn+m233eYvvviiT5kyxf/yl78kHQm4u7/22mtJ/2dJFIvF6mrQl4Vyo91xxx1+4MABd3efN2+efjeQWsvfkcDf/vY3FixYwLZt25g+fToATz/9NLfffnvda8rLy1m5ciWXXXbZab/gs3btWgYPHlzvtwG1Zs2ahbszbtw4AAoKChg7dmyGt0ZS0b9/fxYsWADAhAkTOHnyZMgVnTuy8gOiZhdxhh8Q7d69m4kTJzJ37ty650aOHMmkSZMoKyujqKiII0eOcNFFF7F06VKGDh3Kd7/73WbX0aJFC2pqaojFYjz66KMUFhYyY8YMDh8+nNOHA+3bt6/3MebOnTt59dVXQ6xIclTSHxCFfijQ1MOBjz/+uN5zI0eO9PXr1/uSJUvqXrN06VJfuXLlaYf6yfz617/26upqLywsdMB/9atfubt7dXW1P/HEEzl/ONC9e/d62/PnP//ZAe/Zs6cPHz489PrUcqbl90eExcXFTJs2rcHwfPXq1UyZMoW3336bIUOGMGjQoCavc9q0aZw4cYKpU6fWnWCaNm0ajz/+eN1rqqureeKJJ5gyZUpmNuQs6NGjB9OmTWPatGmMHDky7HIkx+VNCBQVFTFlyhTuueceevbsyf3330/v3r3p1KkTK1eu5IILLmiwzPPPP88jjzxCaWkpTz75ZIMfm8ycOZNHH32Uqqqqur69e/fy5JNP1j3+8ssvefrpp5k4cWL2Ni5N+/bt45FHHuE3v/kNACdPnqRjx44KAGmasA8Fmno4UGvXrl31DgE2btzoq1atSjrMv+OOO/zhhx/2iRMn+qWXXuo/+tGPvLKy0idNmuTjxo3z1q1b173X2LFj/ZlnnnHAx40b5+7uJ0+edMBbtmzp9957b9hDuTO2Dh06+Lhx43z48OE+fPhwd3efPXt26HWp5UxLejiQzh9uF+Ad4hcN2QBMCPqnEr+a0LqgDctkCDTHihUrfNeuXX7dddfVrfO2227zCy64IOn7VVVV+UsvveQvvfSS19TU+OjRo8PeaSm3zp07+9133+0/+9nP8iLA1M5Ky3gIFAPfDu63I345sV7EQ+Bfm7mupEVn6qfEiSFwupb4U9VYLOavvPKKz5s3L+wdl1Lr16+fv/LKKz59+nS/9tprQ69HLSdaZr8n4O6VQGVw/7CZfUb8UuOhevbZZ1m4cCE///nPWblyJUOHDk1pPWbGyJEj665jmG/H1zt27GDTpk106tSJTz75JOxyJIdl5MSgmXUF+gIfBF0Pmdl6M3vezC7OxHs0xXPPPccvf/lLli5dSmVlJSNHjqR79+5NXv7GG2+sHZnUKSws5Oabb850qVm3d+9e1q1bF3YZkg9SPRxIGMq3BdYCI4PHRUAh8YD5FfB8I8uVAGuClnT40tTDgblz5/qAAQO8uLi4btnu3bv7gAEDfMCAAd6uXbsmD5kGDBjQ4CfFuf49gWRt0KBBvnXrVp0YVEtsmT0nEPwhtyR+ReGJjTzfFfi0CetJWnRTQ2Dv3r0+fvz4jP1j9e7d23v37u19+/b148ePe69evcLeec1qAwcO9LKyMnfXpwNq9VrGTwwa8CLwf07pL064/ygwP9sh4O6+b98+37Rpk2/atMnvuuuujP3DnXrtwVxvffv29R07drh7/GfRRUVFodekljMt498YHAT8E3Cjma0L2jBgupn91czWAzcQD4KM+Oijjxg1alSD/tmzZ9OvXz+WLVvGlVdeyYUXXpipt2Tr1q0ZW9fZ0KZNG7p06QLA4cOHNWeCnFE6nw6sJPmcg0tSL6dxGzZsYMiQIVx99dUNnhs1ahRbt27l4MGD2XjrvPLhhx9y5513UlJSwgMPPBB2OZIPUj0cyGSjkeFL7eHAli1b/MILL3TAW7Ro4XfddVeDw4Fjx475sWPH3N193LhxYQ+7Qm0tWrRo1slQtci0/P0BUU1NTd3/8tXV1XXXB0zUpk0b2rRpA8BTTz3FnXfeeVZrzCWN/RuJJJPzIfDFF1/Qt2/fZi3z2GOP6ff0Ik2U8yEQi8U4fvx4vb633nqLH//4x40uc/LkSV17TqSJcjoEKisr+da3vtWg3911uWmRDMnpEADqTR/eVM8++yyjR4/OQjUi556cD4Fkbr31Vv74xz+GXYbIOSEvQwBo8EOfRA888ADz5s07i9WI5K+8DIG33nor0h8BimRSXoYAxL87oGvPi6Qvb0Ng0aJFmhhEJAPyNgQg/n2Ao0ePhl2GSF7L6xBYvHgxU6dODbsMkbyW1yEwbNgwxo8fz4EDB+r6Dhw4wJEjR0KsSiTPhP0LwtP9irCpbcyYMb5582bfv3+/T5gwIexfaqmp5WpL+ivCnJ6QtLnGjx/PV199xZw5czKxOpFzTdIJSdMKATPbDhwGaoBqd+9vZpcAC4hfX3A7cJe7H2hsHcF6wk8ikXNf0hDIxDmBG9y9T8LKJwHL3f1yYHnwWERyVDZODI4AXgjuvwDcnoX3EJEMSTcEHHjbzNaaWUnQV+Tx2YkIbjsmW9DMSsxsjZmtSbMGEUlDyhcaDQxy9woz6wgsM7NNTV3Q3WcBs0DnBETClNZIwN0rgts9wEJgILDbzIoBgts96RYpItmTcgiY2QVm1q72PjAE+BRYDIwJXjYGWJRukSKSPekcDhQBC82sdj1z3f0tM/sQeNnMxgI7AP3mVySHnVNfFhKR08ra9wREJI8pBEQiTiEgEnEKAZGIUwiIRJxCQCTiFAIiEacQEIk4hYBIxCkERCJOISAScQoBkYhTCIhEnEJAJOIUAiIRl/JFRczsSuLzC9TqDvw7cBEwDtgb9P+buy9JuUIRyaqMXFTEzAqBXcDfA/cDR9z9N81YXhcVEcm+rF5U5Cbgc3f/IkPrE5GzJFMhMAqYl/D4ITNbb2bPm9nFGXoPEcmCtEPAzFoB/wj8v6Dr90APoA9QCcxoZDlNPiKSA9I+J2BmI4AH3X1Ikue6Aq+7+9VnWIfOCYhkX9bOCYwm4VCgduKRwB3E5yIQkRyV1jRkZnY+cAvwQEL3dDPrQ3yewu2nPCciOUbzDohEh+YdEJGGFAIiEacQEIk4hYBIxCkERCJOISAScQoBkYhTCIhEnEJAJOIUAiIRpxAQiTiFgEjEKQREIk4hIBJxCgGRiDtjCAQXC91jZp8m9F1iZsvMbGtwe3HCc5PNrNTMNpvZD7NVuIhkRlNGAn8Abj2lbxKw3N0vB5YHjzGzXsSvPHxVsMzvgjkJRCRHnTEE3P1d4KtTukcALwT3XwBuT+if7+4n3L0MKAUGZqhWEcmCVM8JFLl7JUBw2zHo7wzsTHhdedAnIjkqrQuNJmFJ+pJeP9DMSoCSDL+/iDRTqiOB3bWXFg9u9wT95UCXhNddBlQkW4G7z3L3/skufCgiZ0+qIbAYGBPcHwMsSugfZWatzawbcDmwOr0SRSSbzng4YGbzgOuBb5hZOfA/gSeAl81sLLADuBPA3TeY2cvARqCa+MxENVmqXUQyQPMOiESH5h0QkYYUAiIRpxAQiTiFgEjEKQREIk4hIBJxCgGRiFMIiEScQkAk4hQCIhGnEBCJOIWASMQpBEQiTiEgEnEKAZGIUwiIRFyqk4/8bzPbZGbrzWyhmV0U9Hc1s6/NbF3Qnslm8SKSvlQnH1kGXO3u1wBbgMkJz33u7n2C9tPMlCki2ZLS5CPu/ra7VwcP3yd+VWERyUOZOCfwz8CbCY+7mdnHZvafZvb9xhYysxIzW2NmazJQg4ikKK3JR8xsCvGrCs8JuiqBv3P3/WbWD3jNzK5y90OnLuvus4BZwXp0oVGRkKQ8EjCzMcA/AHd7cMniYA7C/cH9tcDnwBWZKFREsiOlEDCzW4GfA//o7scS+i+tnYXYzLoTn3xkWyYKFZHsSHXykclAa2CZmQG8H3wScB3wv8ysGqgBfurup85oLCI5RJOPiESHJh8RkYYUAiIRpxAQiTiFgEjEKQREIk4hIBJxCgGRiFMIiEScQkAk4hQCIhGnEBCJOIWASMQpBEQiTiEgEnEKAZGIS3XegalmtithfoFhCc9NNrNSM9tsZj/MVuEikhmpzjsAMDNhfoElAGbWCxgFXBUs87vay42JSG5Kad6B0xgBzA8uOFoGlAID06hPRLIsnXMCDwXTkD1vZhcHfZ2BnQmvKQ/6GtC8AyK5IdUQ+D3QA+hDfK6BGUG/JXlt0usHuvssd++f7JpnInL2pBQC7r7b3WvcPQY8x38P+cuBLgkvvQyoSK9EEcmmVOcdKE54eAdQ+8nBYmCUmbU2s27E5x1YnV6JIpJNqc47cL2Z9SE+1N8OPADg7hvM7GVgI/HpyR5095rslC4imaB5B0SiQ/MOiEhDCgGRiFMIiEScQkAk4hQCIhGnEBCJOIWASMQpBEQiTiEgEnEKAZGIUwiIRJxCQCTiFAIiEacQEIk4hYBIxKU678CChDkHtpvZuqC/q5l9nfDcM9ksXkTSd8YrCxGfd+D/Ai/Wdrj7j2vvm9kM4GDC6z939z6ZKlBEsuuMIeDu75pZ12TPmZkBdwE3ZrYsETlb0j0n8H1gt7tvTejrZmYfm9l/mtn301y/iGRZUw4HTmc0MC/hcSXwd+6+38z6Aa+Z2VXufujUBc2sBChJ8/1FJE0pjwTMrAUwElhQ2xdMP7Y/uL8W+By4ItnymnxEJDekczhwM7DJ3ctrO8zs0toJSM2sO/F5B7alV6KIZFNTPiKcB/x/4EozKzezscFTo6h/KABwHbDezD4B/gj81N2bOpmpiIRA8w6IRIfmHRCRhhQCIhGnEBCJOIWASMQpBEQiTiEgEnEKAZGIUwiIRJxCQCTiFAIiEacQEIk4hYBIxCkERCJOISAScQoBkYhrykVFupjZO2b2mZltMLMJQf8lZrbMzLYGtxcnLDPZzErNbLOZ/TCbGyAi6WnKSKAa+Bd37wl8B3jQzHoBk4Dl7n45sDx4TPDcKOAq4Fbgd7WXHBOR3HPGEHD3Snf/KLh/GPgM6AyMAF4IXvYCcHtwfwQwP7joaBlQCgzMdOEikhnNOicQTELSF/gAKHL3SogHBdAxeFlnYGfCYuVBn4jkoCbPO2BmbYFXgEfc/VB88qHkL03S1+Aagpp3QCQ3NGkkYGYtiQfAHHd/NejebWbFwfPFwJ6gvxzokrD4ZUDFqevUvAMiuaEpnw4YMBv4zN2fTHhqMTAmuD8GWJTQP8rMWptZN+JzD6zOXMkikklNORwYBPwT8NfaKciBfwOeAF4O5iHYAdwJ4O4bzOxlYCPxTxYedPeajFcuIhmheQdEokPzDohIQwoBkYhTCIhEnEJAJOIUAiIRpxAQiTiFgEjEKQREIk4hIBJxCgGRiFMIiEScQkAk4hQCIhGnEBCJOIWASMQpBEQiTiEgEnEKAZGIa/Ilx7NsH3A0uM1X3yC/64f834Z8rx+yuw3/I1lnTlxjEMDM1uTz5cfzvX7I/23I9/ohnG3Q4YBIxCkERCIul0JgVtgFpCnf64f834Z8rx9C2IacOScgIuHIpZGAiIQg9BAws1vNbLOZlZrZpLDraSoz225mfzWzdWa2Jui7xMyWmdnW4PbisOusZWbPm9keM/s0oa/Res1scrBPNpvZD8Opur5GtmGqme0K9sM6MxuW8FxObYOZdTGzd8zsMzPbYGYTgv5w94O7h9aAQuBzoDvQCvgE6BVmTc2ofTvwjVP6pgOTgvuTgF+HXWdCbdcB3wY+PVO9QK9gX7QGugX7qDBHt2Eq8K9JXptz2wAUA98O7rcDtgR1hrofwh4JDARK3X2bu58E5gMjQq4pHSOAF4L7LwC3h1hLPe7+LvDVKd2N1TsCmO/uJ9y9DCglvq9C1cg2NCbntsHdK939o+D+YeAzoDMh74ewQ6AzsDPhcXnQlw8ceNvM1ppZSdBX5O6VEN/hQMfQqmuaxurNt/3ykJmtDw4XaofSOb0NZtYV6At8QMj7IewQsCR9+fJxxSB3/zYwFHjQzK4Lu6AMyqf98nugB9AHqARmBP05uw1m1hZ4BXjE3Q+d7qVJ+jK+DWGHQDnQJeHxZUBFSLU0i7tXBLd7gIXEh2m7zawYILjdE16FTdJYvXmzX9x9t7vXuHsMeI7/Hi7n5DaYWUviATDH3V8NukPdD2GHwIfA5WbWzcxaAaOAxSHXdEZmdoGZtau9DwwBPiVe+5jgZWOAReFU2GSN1bsYGGVmrc2sG3A5sDqE+s6o9o8ncAfx/QA5uA1mZsBs4DN3fzLhqXD3Qw6c8R1G/Czp58CUsOtpYs3diZ+1/QTYUFs30AFYDmwNbi8Ju9aEmucRHy5XEf8fZuzp6gWmBPtkMzA07PpPsw3/AfwVWB/80RTn6jYAg4kP59cD64I2LOz9oG8MikRc2IcDIhIyhYBIxCkERCJOISAScQoBkYhTCIhEnEJAJOIUAiIR91/cNDFOJe961gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = './train_data/abnormal/06f1a458-feb8-45e4-82c9-3340942244c3-DepBack-09_36_41.791-S10-W16.2.png'\n",
    "new_image = load_image(img_path,show=True)\n",
    "pred = model.predict(new_image)\n",
    "predict=np.argmax(pred,axis=1)\n",
    "maxindex = int(np.argmax(pred))\n",
    "maxindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2564\n"
     ]
    }
   ],
   "source": [
    "normal_result = []\n",
    "error_normal = []\n",
    "path = \"./train_data/normal/\"\n",
    "\n",
    "for image_file in os.listdir(path):\n",
    "\n",
    "    new_image = load_image(path+image_file)\n",
    "    pred = model.predict(new_image)\n",
    "    predict=np.argmax(pred,axis=1)\n",
    "    normal_result.append(predict[0])\n",
    "\n",
    "print(normal_result.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_result = []\n",
    "error = []\n",
    "path = \"./train_data/abnormal/\"\n",
    "\n",
    "for image_file in os.listdir(path):\n",
    "#     print(image_file)\n",
    "    new_image = load_image(path+image_file)\n",
    "    pred = model.predict(new_image)\n",
    "    predict=np.argmax(pred,axis=1)\n",
    "    abnormal_result.append(predict[0])\n",
    "    if predict[0] == 0:\n",
    "        error.append(image_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2564\n",
      "0\n",
      "845\n"
     ]
    }
   ],
   "source": [
    "print(normal_result.count(1))\n",
    "print(len(normal_result))\n",
    "print(abnormal_result.count(1))\n",
    "print(len(abnormal_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mobile_v2_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "# model.evaluate_generator(generator=train_datagen,\n",
    "#                          steps=900,\n",
    "#                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
